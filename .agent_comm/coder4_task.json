{
  "agent_id": "coder4",
  "task_id": "task_2",
  "files": [
    {
      "name": "requirements.txt",
      "purpose": "Python dependencies",
      "priority": "high"
    },
    {
      "name": "preprocessing.py",
      "purpose": "Image preprocessing utilities",
      "priority": "medium"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.AI_2508.10869v1_Medico_2025_Visual_Question_Answering_for_Gastroi",
    "project_type": "computer_vision",
    "description": "Enhanced AI project based on cs.AI_2508.10869v1_Medico-2025-Visual-Question-Answering-for-Gastroi with content analysis. Detected project type: computer vision (confidence score: 7 matches).",
    "key_algorithms": [
      "Evaluation",
      "Their",
      "Its",
      "Rationale-Guided",
      "Learning",
      "Disease",
      "Qwen3-30B-A3B",
      "Tool"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "PDF: cs.AI_2508.10869v1_Medico-2025-Visual-Question-Answering-for-Gastroi.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nMedico 2025:\nVisual Question Answering for Gastrointestinal Imaging\nSushant Gautama,c, Vajira Thambawitaa, Michael Rieglerb, P\u00e5l Halvorsena,c,\nSteven Hicksa\naSimulaMet - Simula Metropolitan Center for Digital Engineering, Oslo, Norway\nbSimula Research Laboratory, Oslo, Norway\ncOsloMet - Oslo Metropolitan University, Oslo, Norway\nAbstract\nThe Medico 2025 challenge addresses Visual Question Answering (VQA) for\nGastrointestinal (GI) imaging, organized as part of the MediaEval tasks series.\nThe challenge focuses on developing Explainable Artificial Intelligence (XAI)\nmodels that answer clinically relevant questions based on GI endoscopy images\nwhile providing interpretable justifications aligned with medical reasoning. It\nintroduces two subtasks: (1) answering diverse types of visual questions using\nthe Kvasir-VQA-x1 dataset, and (2) generating multimodal explanations to\nsupport clinical decision-making. The Kvasir-VQA-x1 dataset, created from\n6,500 images and 159,549 complex question\u2013answer (QA) pairs, serves as the\nbenchmark for the challenge. By combining quantitative performance metrics\nand expert-reviewed explainability assessments, this task aims to advance\ntrustworthy Artificial Intelligence (AI) in medical image analysis. Instructions,\ndata access, and an updated guide for participation are available in the official\ncompetition repository: github.com/simula/MediaEval-Medico-2025\n1. Introduction\nGastrointestinal (GI) diseases are among the most common and critical\nhealth concerns worldwide, with conditions like Colorectal Cancer (CRC)\nrequiring early diagnosis and intervention [ 13,15]. AI-driven decision support\nsystems [ 1,2] have shown potential in assisting clinicians with diagnosis, but\na major challenge remains: explainability. While deep learning models can\nachievehighdiagnosticaccuracy, their\u201cblack-box\u201dnaturelimitstheiradoption\nin clinical practice, where trust and interpretability are essential [4, 12].arXiv:2508.10869v1  [cs.CV]  14 Aug 2025\n\n--- Page 2 ---\nAfter successfully organizing multiple Medico challenges at MediaEval\nin previous years, for the new edition1we propose the Medico 2025: Visual\nQuestion Answering (with multimodal explanations) for Gastrointestinal\nImaging.\nMedical VQA is a rapidly growing research area that combines computer\nvision and natural language processing to answer clinically relevant questions\nbased on medical images [ 4]. However, existing VQA models often lack\ntransparency, making it difficult for healthcare professionals to assess the\nreliability of AI-generated answers [ 4,12]. To address this, the Medico\n2025 challenge will focus on explainable VQA for GI imaging, encouraging\nparticipants to develop models that provide not only accurate answers but\nalso clear justifications aligned with clinical reasoning.\nThe challenge provides a benchmark dataset of GI images, videos, and\nassociated VQA annotations, enabling rigorous evaluation of AI models. By\nintegrating multimodal data and explainability metrics, we aim to advance\nresearch in interpretable AI and increase the potential for clinical adoption.\nWe define two main subtasks for this year\u2019s challenge. Subtask 2 builds\non Subtask 1, meaning Subtask 1 must be completed in order to participate\nin Subtask 2.\n\u2022Subtask 1: AI Performance on Medical Image Question Answering\nThis subtask challenges participants to develop AI models that\naccurately interpret and respond to clinical questions based on GI\nimages from the Kvasir-VQA-x1 dataset, which retains the original\n6,500 images from Kvasir-VQA [ 7] but expands them to 159,549 QA\npairs across multiple conditions and instruments. Questions fall into\nsix categories: Yes/No, Single-Choice, Multiple-Choice, Color-Related,\nLocation-Related, and Numerical Count, requiring models to process\nboth visual and textual information.\nPerformance will be assessed using Bilingual Evaluation Understudy\n(BLEU), Recall-Oriented Understudy for Gisting Evaluation\n(ROUGE)-1/2/L, and Metric for Evaluation of Translation with\nExplicit ORdering (METEOR).\n1https://multimediaeval.github.io/editions/2025/tasks/medico\nii\n\n--- Page 3 ---\n\u2022Subtask 2: Clinician-Oriented Multimodal Explanations in GI\nThis subtask builds upon Subtask 1, requiring participants to justify\ntheir model\u2019s predictions using multiple complementary forms of\nreasoning. The goal is to generate rich, multimodal explanations that\nare transparent, understandable, and trustworthy to clinicians [ 10]. At\na minimum, explanations must include a detailed textual narrative\nin clinical language that directly supports the predicted answer [ 6].\nParticipants are strongly encouraged to provide an accompanying visual\nexplanation\u2014such as a heatmap, segmentation mask, or bounding\nbox\u2014that clearly links to the textual reasoning and highlights the\nrelevant finding [ 11,14,5]. Confidence scores, indicating the model\u2019s\ncertainty, are optional but recommended.\nAll outputs will be human-evaluated by domain experts and medical\nprofessionals, using predefined criteria for clarity, coherence between\nmodalities, andmedicalrelevance, toassesshowwelltheoutputssupport\nclinical decision-making.\nMedical AI systems must be both accurate and interpretable to be useful\nin clinical practice. While deep learning models have shown great potential in\ndiagnosing GI conditions from medical images, their adoption remains limited\ndue to a lack of transparency. Clinicians need to understand why an AI\nsystem makes a specific decision, especially when it comes to critical medical\ndiagnoses. XAI methods aim to bridge this gap by providing justifications\nthat align with clinical reasoning, improving trust, reliability, and ultimately\npatient outcomes.\nThis challenge builds on previous work in medical VQA, where AI models\nanswer clinically relevant questions based on GI images. However, traditional\nVQAmodelsoftenprovideanswerswithoutexplanations, makingitdifficultfor\nmedical professionals to assess their validity. By incorporating explainability\ninto the task, we encourage the development of models that not only provide\naccurateresponsesbutalsooffermeaningfulinsightsintotheirdecision-making\nprocess. This will help ensure that AI systems can be safely integrated into\nclinical workflows, assisting rather than replacing human expertise.\niii\n\n--- Page 4 ---\n2. Data\nThe Medico 2025 challenge builds on the Kvasir-VQA-x1 dataset [ 8],\na substantial extension of the original Kvasir-VQA [ 7]. It comprises 6,500\nGI endoscopic images from HyperKvasir [ 3] and Kvasir-Instrument [ 9], paired\nwith159,549 QA pairs stratified by reasoning complexity.\nTable 1: An example image with one representative question\u2013answer pair from each\ncomplexity level from the Kvasir-VQA-x1 dataset. Each image in the dataset may have\nmultiple QA pairs at every level.\nComplexity Question Answer Question Class\n1 Which anatomical landmark is\nvisible in the image?No identifiable anatomical\nlandmark presentlandmark_location\n2 What procedure is depicted in\nthe image and what colors are\nassociated with the abnormality?Evidence of colonoscopy findings\nwith pink and red mucosal lesionsprocedure_type,\nabnormality_color\n3 Are there any anatomical\nlandmarks visible, what type of\npolyps are present, and what\ncolors are the observed\nabnormalities?No anatomical landmarks\nidentified, no polyps observed, and\nmultiple abnormalities with pink\nand red coloration.landmark_presence,\npolyp_type,\nabnormality_color\n2.1. Dataset Composition\nEach image is paired with multiple QA entries generated by merging one\nto three atomic QA pairs using the Qwen3-30B-A3B model [ 16]. The resulting\nnatural-language questions are fluently phrased, and each entry is annotated\nwith a complexity score (ranging from 1 to 3) and a question_class label\nspecifying its clinical category. Examples of these classes include polyp_type ,\ninstrument_presence , and finding_count .\niv\n\n--- Page 5 ---\n2.2. Question Complexity and Clinical Categorization\nThe dataset supports stratified evaluation by QA complexity:\n\u2022Level 1: Questions derived from a single atomic QA (approximately\n34.4%).\n\u2022Level 2: Reasoning over two merged atomic QA (32.8%).\n\u2022Level 3: Synthesis across three atomic QA (32.8%).\nEach QA pair is assigned one or more question_class labels to support\nfine-grained analysis across clinical categories such as pathology, anatomical\nlocalization, procedural context, and visual findings.\nPublic Availability and Format\nKvasir-VQA-x1 is hosted at https://huggingface.co/datasets/\nSimulaMet/Kvasir-VQA-x1 . The dataset includes img_id(same as in\nKvasir-VQA [ 7]),complexity ,question ,answer,original (atomic QA\ncomponents), and question_class . It is split into training and testing\nsubsets for reproducible experimentation. While only the original images are\nreleased, we encourage applying weak augmentations (e.g., rotation, color\njitter, crop) when fine-tuning models with the dataset.\n3. Evaluation\nThe Medico 2025 challenge evaluates both the accuracy andclinical\ninterpretability of medical VQA models, emphasizing not only correct\nanswers but also their relevance and explanatory quality in the context\nof GI diagnostics.\n3.1. Subtask 1: GI Question Answering\nThis subtask evaluates how effectively models answer clinically relevant\nGI questions from medical images, emphasizing both predictive accuracy\nandreasoning depth . Performance is measured using language quality\nmetrics\u2014BLEU, ROUGE-1/2/L, and METEOR\u2014to assess alignment with\nreference responses. Evaluation is conducted in two settings: an original\nsettingwithcleanimagesanda transformed settingthatappliesaugmentations\nfor robustness testing. Criteria include accuracy, relevance, and medical\ncorrectness.\nv\n\n--- Page 6 ---\nEvaluation is stratified across three levels: (1) overall performance ,\naggregating scores across all categories and complexities; (2) category-level\nanalysis over 18 question types (e.g., polyp type, instrument presence),\nwith visualizations such as radar plots and rank-normalized heatmaps; and\n(3)complexity-level evaluation , distinguishing between factual (Level 1),\nmoderately inferential (Level 2), and higher-order reasoning prompts (Level\n3).\nThis structured, multi-dimensional evaluation framework provides a\ncomprehensive assessment of both correctness and clinical reasoning, which is\ncritical for robust deployment in medical settings.\n3.2. Subtask 2: Clinician-Oriented Explanation Quality Assessment\nThis subtask extends Subtask 1 by requiring participants to justify their\npredictions through detailed, multimodal explanations. The objective is to\nmove beyond providing an answer and produce outputs that are transparent,\nclinically relevant, and aligned with the model\u2019s own reasoning.\nEvaluation in Subtask 2 will be conducted entirely by human\nexperts and medical professionals. Automated metrics from Subtask 1\n(e.g., BLEU, ROUGE-1/2/L, METEOR) are used only to assess answer\ncorrectness; explanation quality will be judged through expert review\naccording to the criteria below.\nEach explanation must combine:\n\u2022Textual Explanation (Mandatory): A detailed, clinician-oriented\nnarrative that justifies the predicted answer using multiple aspects.\n\u2022Visual Explanation (Optional but Highly Encouraged):\nA supporting visual modality\u2014such as a Grad-CAM heatmap,\nsegmentation mask, or bounding box\u2014that highlights the region(s)\nreferenced in the textual explanation. Visuals must clearly link to and\nreinforce the textual reasoning.\n\u2022Confidence Score (Optional): A scalar in [0, 1] indicating the\nmodel\u2019s certainty in its prediction, derived from softmax probabilities,\ncalibrated uncertainty, or Bayesian methods.\nSubmissions must follow a structured JSON and expert reviewers will rate\nsubmissions based on (but not limited to) the following criteria:\n\u2022Clarity: Ease of understanding for a clinician.\nvi\n\n--- Page 7 ---\n\u2022Coherence: Logical consistency between visual and textual\ncomponents.\n\u2022Medical Relevance: Consistency with established clinical knowledge.\n\u2022Visual Alignment: Whether visual elements accurately highlight the\nrelevant findings.\nBy combining accurate predictions with interpretable, clinically grounded\njustifications, this subtask aims to promote AI systems that can be\nmeaningfully integrated into real-world diagnostic workflows.\n4. Discussion and Outlook\nThe Medico 2025 challenge marks an important step toward bridging the\ngap between powerful deep learning models and their practical adoption in\nclinical settings. By focusing on explainable VQA for GI imaging, this task\npromotes the development of interpretable AI models that not only generate\naccurate responses but also provide transparent justifications aligned with\nmedical reasoning.\nParticipants are encouraged to innovate beyond traditional accuracy\nmetrics and embrace multimodal explainability as a core component of their\nsolutions. The availability of the Kvasir-VQA-x1 dataset, tailored for this\ntask, will support reproducible research and enable robust benchmarking.\nLooking ahead, we anticipate that methods developed for Medico 2025\nwill inspire broader applications of explainable AI in other medical domains.\nBy fostering interdisciplinary collaboration between the AI and medical\ncommunities, this challenge aims to pave the way for clinically viable AI tools\nthat are both trusted and actionable in real-world healthcare scenarios.\nReferences\n[1]Hassam Ali, Muhammad Ali Muzammil, Dushyant Singh Dahiya, Farishta\nAli, Shafay Yasin, Waqar Hanif, Manesh Kumar Gangwani, Muhammad\nAziz, Muhammad Khalaf, Debargha Basuli, et al. Artificial intelligence in\ngastrointestinalendoscopy: acomprehensivereview. Annals of gastroenterology ,\n37(2):133, 2024.\nvii\n\n--- Page 8 ---\n[2]M Alvaro Berb\u00eds, Jos\u00e9 Aneiros-Fern\u00e1ndez, F Javier Mendoza Olivares, Enrique\nNava, and Antonio Luna. Role of artificial intelligence in multidisciplinary\nimagingdiagnosisofgastrointestinaldiseases. World journal of gastroenterology ,\n27(27):4395, 2021.\n[3]Hanna Borgli, Vajira Thambawita, Pia H Smedsrud, Steven Hicks, Debesh Jha,\nSigrun L Eskeland, Kristin Ranheim Randel, Konstantin Pogorelov, Mathias\nLux, Duc Tien Dang Nguyen, et al. Hyperkvasir, a comprehensive multi-class\nimage and video dataset for gastrointestinal endoscopy. Scientific data , 7(1):\n283, 2020.\n[4]Katarzyna Borys, Yasmin Alyssa Schmitt, Meike Nauta, Christin Seifert,\nNicole Kr\u00e4mer, Christoph M. Friedrich, and Felix Nensa. Explainable AI in\nmedicalimaging: Anoverviewforclinicalpractitioners\u2013Beyondsaliency-based\nXAI approaches. Eur. J. Radiol. , 162, May 2023. ISSN 0720-048X. doi:\n10.1016/j.ejrad.2023.110786.\n[5]Fadl Dahan, Jamal Hussain Shah, Rabia Saleem, Muhammad Hasnain, Maira\nAfzal, and Taha M. Alfakih. A hybrid XAI-driven deep learning framework\nfor robust GI tract disease diagnosis. Sci. Rep. , 15(21139):1\u201318, July 2025.\nISSN 2045-2322. doi: 10.1038/s41598-025-07690-3.\n[6]Xiaotang Gai, Chenyi Zhou, Jiaxiang Liu, Yang Feng (xn-27q. xn-6xw. ),\nJian Wu, and Zuozhu Liu. MedThink: A Rationale-Guided Framework\nfor Explaining Medical Visual Question Answering. ACL Anthology , pages\n7438\u20137450, April 2025. doi: 10.18653/v1/2025.findings-naacl.415.\n[7]Sushant Gautam, Andrea M Stor\u00e5s, Cise Midoglu, Steven A Hicks, Vajira\nThambawita, P\u00e5l Halvorsen, and Michael A Riegler. Kvasir-vqa: A text-image\npair gi tract dataset. In Proceedings of the First International Workshop on\nVision-Language Models for Biomedical Applications , pages 3\u201312, 2024.\n[8]Sushant Gautam, Michael A. Riegler, and P\u00e5l Halvorsen. Kvasir-VQA-x1:\nA Multimodal Dataset for Medical Reasoning and Robust MedVQA in\nGastrointestinal Endoscopy. arXiv, June 2025. doi: 10.48550/arXiv.2506.\n09958.\n[9]Debesh Jha, Sharib Ali, Krister Emanuelsen, Steven A Hicks, Vajira\nThambawita, Enrique Garcia-Ceja, Michael A Riegler, Thomas De Lange,\nPeter T Schmidt, H\u00e5vard D Johansen, et al. Kvasir-instrument: Diagnostic\nand therapeutic tool segmentation dataset in gastrointestinal endoscopy. In\nMultiMedia Modeling: 27th International Conference, MMM 2021, Prague,\nviii\n\n--- Page 9 ---\nCzech Republic, June 22\u201324, 2021, Proceedings, Part II 27 , pages 218\u2013229.\nSpringer, 2021.\n[10]Dost Muhammad and Malika Bendechache. Unveiling the black box: A\nsystematic review of Explainable Artificial Intelligence in medical image\nanalysis. Comput. Struct. Biotechnol. J. , 24:542\u2013560, December 2024. ISSN\n2001-0370. doi: 10.1016/j.csbj.2024.08.005.\n[11]Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Anna Rohrbach, Bernt\nSchiele, Trevor Darrell, and Marcus Rohrbach. Multimodal Explanations:\nJustifying Decisions and Pointing to the Evidence . IEEE Computer Society,\nJune 2018. ISBN 978-1-5386-6420-9. doi: 10.1109/CVPR.2018.00915.\n[12]Zohaib Salahuddin, Henry C. Woodruff, Avishek Chatterjee, and Philippe\nLambin. Transparency of deep neural networks for medical image analysis: A\nreview of interpretability methods. Comput. Biol. Med. , 140:105111, January\n2022. ISSN 0010-4825. doi: 10.1016/j.compbiomed.2021.105111.\n[13]Arjun Singh. Global burden of five major types of gastrointestinal cancer.\nGastroenterology Rev. , 19(3):236\u2013254, 2024. ISSN 1895-5770. doi: 10.5114/pg.\n2024.141834.\n[14]Andrea M. Stor\u00e5s, Maximilian Dreyer, Frederik Pahde, Sebastian Lapuschkin,\nWojciech Samek, P\u00e5l Halvorsen, Thomas de Lange, Yuichi Mori, Alexander\nHann, Tyler M. Berzin, Sravanthi Parasa, and Michael A. Riegler. Exploring\nthe clinical value of concept-based AI explanations in gastrointestinal disease\ndetection. Sci. Rep. , 15(28860):1\u201311, August 2025. ISSN 2045-2322. doi:\n10.1038/s41598-025-14408-y.\n[15]Rui Wang, Zhaoqi Li, Shaojun Liu, and Decai Zhang. Global, regional, and\nnational burden of 10 digestive diseases in 204 countries and territories from\n1990 to 2019. Frontiers in public health , 11:1061453, 2023.\n[16]An Yang, Anfeng Li, Baosong Yang, et al. Qwen3 Technical Report. arXiv,\nMay 2025. doi: 10.48550/arXiv.2505.09388.\nix",
  "project_dir": "artifacts/projects/enhanced_cs.AI_2508.10869v1_Medico_2025_Visual_Question_Answering_for_Gastroi",
  "communication_dir": "artifacts/projects/enhanced_cs.AI_2508.10869v1_Medico_2025_Visual_Question_Answering_for_Gastroi/.agent_comm",
  "assigned_at": "2025-08-16T20:57:06.362047",
  "status": "assigned"
}